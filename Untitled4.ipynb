{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIyFNEJ0uhbWY3edxM/nau",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saran167/Loss-function-/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "TSQ_qOkAlLD8",
        "outputId": "ae738f29-8d1e-49c9-fa9e-3ecec492347a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-54cc5527c42a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# One-hot encode target labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Loss function\n",
        "def loss_function(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Hebbian Neural Network class\n",
        "class HebbianNN:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = sigmoid(np.dot(X, self.weights))\n",
        "        return self.output\n",
        "\n",
        "    def hebbian_update(self):\n",
        "        self.weights += self.learning_rate * np.dot(self.input.T, self.output)\n",
        "\n",
        "    def train(self, X, y, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = loss_function(y, y_pred)\n",
        "            self.hebbian_update()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Load Digits dataset\n",
        "data = load_digits()\n",
        "X = data.data  # Extract features (64 features per sample)\n",
        "y = data.target.reshape(-1, 1)  # Reshape target values for encoding\n",
        "\n",
        "# One-hot encode target labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train the Hebbian Neural Network\n",
        "nn = HebbianNN(input_size=X.shape[1], output_size=y.shape[1], learning_rate=0.01)\n",
        "nn.train(X, y, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Define activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define loss function\n",
        "def loss_function(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Hebbian Neural Network class\n",
        "class HebbianNN:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = sigmoid(np.dot(X, self.weights))\n",
        "        return self.output\n",
        "\n",
        "    def hebbian_update(self):\n",
        "        self.weights += self.learning_rate * np.dot(self.input.T, self.output)\n",
        "\n",
        "    def train(self, X, y, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = loss_function(y, y_pred)\n",
        "            self.hebbian_update()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Load Digits dataset\n",
        "data = load_digits()\n",
        "X = data.data  # Extract features (8x8 pixel values)\n",
        "y = data.target.reshape(-1, 1)  # Reshape target values for encoding\n",
        "\n",
        "# One-hot encode target values\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train the Hebbian Neural Network\n",
        "nn = HebbianNN(input_size=X.shape[1], output_size=y.shape[1], learning_rate=0.01)\n",
        "nn.train(X, y, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2to8_HXnnyBp",
        "outputId": "1889d37b-520a-45fb-afab-a8e7c3fa7503"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8d1dd15c68fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# One-hot encode target values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "    def loss_function(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "    class HebbianNN:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = sigmoid(np.dot(X, self.weights))\n",
        "        return self.output\n",
        "\n",
        "    def hebbian_update(self):\n",
        "        self.weights += self.learning_rate * np.dot(self.input.T, self.output)\n",
        "\n",
        "    def train(self, X, y, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = loss_function(y, y_pred)\n",
        "            self.hebbian_update()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "            data = fetch_california_housing()\n",
        "X = data.data  # Extract features (housing data)\n",
        "y = data.target.reshape(-1, 1)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "target_scaler = MinMaxScaler()\n",
        "y = target_scaler.fit_transform(y)\n",
        "nn = HebbianNN(input_size=X.shape[1], output_size=y.shape[1], learning_rate=0.01)\n",
        "nn.train(X, y, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "bZMlV2rOoKBD",
        "outputId": "d31d4763-dfe5-4b1a-dce5-6533f1509ba2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 6 (<ipython-input-5-63f8ee8b4d9e>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-63f8ee8b4d9e>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    return np.mean((y_true - y_pred) ** 2)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Define activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define loss function (Mean Squared Error)\n",
        "def loss_function(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Hebbian Neural Network class\n",
        "class HebbianNN:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = sigmoid(np.dot(X, self.weights))\n",
        "        return self.output\n",
        "\n",
        "    def hebbian_update(self):\n",
        "        self.weights += self.learning_rate * np.dot(self.input.T, self.output)\n",
        "\n",
        "    def train(self, X, y, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = loss_function(y, y_pred)\n",
        "            self.hebbian_update()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Flatten images (32x32x3 -> 3072 features)\n",
        "X_train = X_train.reshape(X_train.shape[0], -1).astype(np.float32)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).astype(np.float32)\n",
        "\n",
        "# Normalize pixel values (0 to 1)\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Hebbian Neural Network on CIFAR-10\n",
        "nn = HebbianNN(input_size=X_train.shape[1], output_size=y_train.shape[1], learning_rate=0.01)\n",
        "nn.train(X_train, y_train, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJYeG0KMpHB6",
        "outputId": "e40994fe-c8b5-4b0a-d3a4-0f14f394f40e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Epoch 1, Loss: 0.2644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-cbda8ec52678>:8: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 0.4922\n",
            "Epoch 3, Loss: 0.4916\n",
            "Epoch 4, Loss: 0.4907\n",
            "Epoch 5, Loss: 0.4908\n",
            "Epoch 6, Loss: 0.4909\n",
            "Epoch 7, Loss: 0.4908\n",
            "Epoch 8, Loss: 0.4909\n",
            "Epoch 9, Loss: 0.4910\n",
            "Epoch 10, Loss: 0.4909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Define activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define loss function (Mean Squared Error)\n",
        "def loss_function(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Hebbian Neural Network class\n",
        "class HebbianNN:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = sigmoid(np.dot(X, self.weights))\n",
        "        return self.output\n",
        "\n",
        "    def hebbian_update(self):\n",
        "        self.weights += self.learning_rate * np.dot(self.input.T, self.output)\n",
        "\n",
        "    def train(self, X, y, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = loss_function(y, y_pred)\n",
        "            self.hebbian_update()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Flatten images (32x32x3 -> 3072 features)\n",
        "X_train = X_train.reshape(X_train.shape[0], -1).astype(np.float32)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).astype(np.float32)\n",
        "\n",
        "# Normalize pixel values (0 to 1)\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "# One-hot encode labels (Fix: Use `sparse=False` instead of `sparse_output=False`)\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the Hebbian Neural Network on CIFAR-10\n",
        "nn = HebbianNN(input_size=X_train.shape[1], output_size=y_train.shape[1], learning_rate=0.01)\n",
        "nn.train(X_train, y_train, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5OoNQUfup_6I",
        "outputId": "caaa70aa-90e4-4a33-8486-1def9106b5ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9e878975ca9b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# One-hot encode labels (Fix: Use `sparse=False` instead of `sparse_output=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    }
  ]
}